"""
A module for obtaining repo readme and language data from the github API.

Before using this module, read through it, and follow the instructions marked
TODO.

After doing so, run it like this:

    python acquire.py

To create the `data.json` file that contains the data.
"""
import os
import json
from typing import Dict, List, Optional, Union, cast
import requests

from env import github_token, github_username

# TODO: Make a github personal access token.
#     1. Go here and generate a personal access token: https://github.com/settings/tokens
#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked
#     2. Save it in your env.py file under the variable `github_token`
# TODO: Add your github username to your env.py file under the variable `github_username`
# TODO: Add more repositories to the `REPOS` list below.

REPOS = ['huggingface/transformers',
 'apachecn/ailearning',
 'google-research/bert',
 'hankcs/HanLP',
 'explosion/spaCy',
 'RasaHQ/rasa',
 'huggingface/datasets',
 'mindsdb/mindsdb',
 'RaRe-Technologies/gensim',
 'flairNLP/flair',
 'microsoft/unilm',
 'nltk/nltk',
 'PaddlePaddle/PaddleHub',
 'allenai/allennlp',
 'chiphuyen/stanford-tensorflow-tutorials',
 'PaddlePaddle/PaddleNLP',
 'deepset-ai/haystack',
 'sloria/TextBlob',
 'ymcui/Chinese-BERT-wwm',
 'ymcui/Chinese-LLaMA-Alpaca',
 'brightmart/text_classification',
 'jadore801120/attention-is-all-you-need-pytorch',
 'Morizeyao/GPT2-Chinese',
 'PaddlePaddle/models',
 'stanfordnlp/stanza',
 'NVIDIA/NeMo',
 'MycroftAI/mycroft-core',
 'deeppavlov/DeepPavlov',
 'microsoft/nlp-recipes',
 'zihangdai/xlnet',
 'PaddlePaddle/ERNIE',
 'codertimo/BERT-pytorch',
 'vi3k6i5/flashtext',
 'jessevig/bertviz',
 'chatopera/Synonyms',
 'HIT-SCIR/ltp',
 'MaartenGr/BERTopic',
 'neuml/txtai',
 'snipsco/snips-nlu',
 'pytorch/text',
 'thunlp/OpenPrompt',
 'miso-belica/sumy',
 'CLUEbenchmark/CLUEDatasetSearch',
 'ownthink/Jiagu',
 'IntelLabs/nlp-architect',
 'princeton-nlp/SimCSE',
 'jbesomi/texthero',
 'tensorflow/lingvo',
 'explosion/thinc',
 'yangjianxin1/GPT2-chitchat',
 'huggingface/knockknock',
 'dmlc/gluon-nlp',
 'whitead/paper-qa',
 'BrikerMan/Kashgari',
 'QData/TextAttack',
 'jaymody/picoGPT',
 'RasaHQ/rasa_core',
 'modelscope/modelscope',
 'yanshengjia/ml-road',
 'google-research/electra',
 'readbeyond/aeneas',
 'PetrochukM/PyTorch-NLP',
 'explosion/spacy-course',
 'Separius/awesome-sentence-embedding',
 'google-research/uda',
 'chiphuyen/lazynlp',
 'namisan/mt-dnn',
 'crownpku/Information-Extraction-Chinese',
 'JasonKessler/scattertext',
 'asappresearch/sru',
 'duoergun0729/nlp',
 'chartbeat-labs/textacy',
 'lonePatient/awesome-pretrained-chinese-nlp-models',
 'SCIR-HI/Huatuo-Llama-Med-Chinese',
 'dongrixinyu/JioNLP',
 'DerwenAI/pytextrank',
 'ivan-bilan/The-NLP-Pandect',
 'blmoistawinde/HarvestText',
 'argosopentech/argos-translate',
 'NLP-LOVE/Introduction-NLP',
 'argilla-io/argilla',
 'promptslab/Promptify',
 'clovaai/donut',
 'zjunlp/DeepKE',
 'luopeixiang/named_entity_recognition',
 'songyouwei/ABSA-PyTorch',
 'GauravBh1010tt/DeepLearn',
 'bigscience-workshop/promptsource',
 'lonePatient/BERT-NER-Pytorch',
 'ChineseGLUE/ChineseGLUE',
 'alibaba/AliceMind',
 'shibing624/text2vec',
 'imcaspar/gpt2-ml',
 'deepset-ai/FARM',
 'lukalabs/cakechat',
 'Franck-Dernoncourt/NeuroNER',
 'neuralmagic/sparseml',
 'huggingface/transfer-learning-conv-ai',
 'TingFree/NLPer-Arsenal',
 'OFA-Sys/Chinese-CLIP',
 'PaddlePaddle/Research',
 'fhamborg/news-please',
 'Roshanson/TextInfoExp',
 'alibaba/EasyNLP',
 'plasticityai/magnitude',
 'graph4ai/graph4nlp',
 'adapter-hub/adapter-transformers',
 'ymcui/Chinese-XLNet',
 'Delta-ML/delta',
 'timoschick/pet',
 'yongzhuo/Keras-TextClassification',
 'nyu-mll/jiant',
 'allenai/bi-att-flow',
 'microsoft/LMOps',
 'CVI-SZU/Linly',
 'explosion/sense2vec',
 'neuralmagic/deepsparse',
 'allenai/RL4LMs',
 'yongzhuo/nlp_xiaojiang',
 'jasonwei20/eda_nlp',
 'HIT-SCIR/ELMoForManyLangs',
 'datamade/usaddress',
 '425776024/nlpcda',
 'mosaicml/llm-foundry',
 'airaria/TextBrewer',
 'deepdoctection/deepdoctection',
 'allenai/scispacy',
 'DragonComputer/Dragonfire',
 'juand-r/entity-recognition-datasets',
 'konlpy/konlpy',
 'explosion/spacy-models',
 'allenai/scibert',
 'ymcui/Chinese-ELECTRA',
 'chrismattmann/tika-python',
 'thunlp/TAADpapers',
 'gnes-ai/gnes',
 'yao8839836/text_gcn',
 'explosion/spacy-transformers',
 'linkedin/detext',
 'code-kern-ai/refinery',
 'milvus-io/bootcamp',
 'Maluuba/nlg-eval',
 'capitalone/DataProfiler',
 'summanlp/textrank',
 'SkalskiP/courses',
 'huggingface/hmtl',
 'RubensZimbres/Repo-2017',
 'yuanzhoulvpi2017/zero_nlp',
 'huggingface/text-generation-inference',
 'undertheseanlp/underthesea',
 'bheinzerling/bpemb',
 'nltk/nltk_data',
 'explosion/projects',
 'chenyuntc/PyTorchText',
 'huggingface/course',
 'natasha/natasha',
 'uber-research/PPLM',
 'MilaNLProc/contextualized-topic-models',
 'makcedward/nlp',
 'localminimum/QANet',
 'adbar/trafilatura',
 'uber-archive/plato-research-dialogue-system',
 'brightmart/bert_language_understanding',
 'DengBoCong/nlp-paper',
 'neuml/paperai',
 'liucongg/GPT2-NewsTitle',
 'google/budoux',
 'kakaobrain/kogpt',
 'yoshitomo-matsubara/torchdistill',
 'beir-cellar/beir',
 'atulkum/pointer_summarizer',
 'autoliuweijie/K-BERT',
 'obsei/obsei',
 'wikipedia2vec/wikipedia2vec',
 'lovit/soynlp',
 'graykode/gpt-2-Pytorch',
 'NVIDIA-Merlin/Transformers4Rec',
 'carpedm20/MemN2N-tensorflow',
 'tensorlayer/seq2seq-chatbot',
 'salesforce/CodeT5',
 'Separius/BERT-keras',
 'jfilter/clean-text',
 'lixiang0/WEB_KG',
 'sberbank-ai-lab/LightAutoML',
 'smilelight/lightNLP',
 'MorvanZhou/NLP-Tutorials',
 'ShawnyXiao/TextClassification-Keras',
 'goru001/inltk',
 'nikitakit/self-attentive-parser',
 'williamSYSU/TextGAN-PyTorch',
 'robocorp/rpaframework',
 'cltk/cltk',
 'grammarly/gector',
 'lonePatient/Bert-Multi-Label-Text-Classification',
 'carpedm20/lstm-char-cnn-tensorflow',
 'Tongjilibo/bert4torch',
 'yaserkl/RLSeq2Seq',
 'hyperonym/basaran',
 'alvations/pywsd',
 'huggingface/naacl_transfer_learning_tutorial',
 'openeventdata/mordecai',
 'princeton-nlp/PURE',
 'lonePatient/albert_pytorch',
 'nlp-uoregon/trankit',
 'explosion/spacy-stanza',
 'Tencent/PatrickStar',
 'inspirehep/magpie',
 'ggeop/Python-ai-assistant',
 'soskek/bookcorpus',
 'Santosh-Gupta/SpeedTorch',
 'koursaros-ai/nboost',
 'Decalogue/chat',
 'explosion/spacy-streamlit',
 'AmoDinho/datacamp-python-data-science-track',
 'thunlp/OpenDelta',
 'wyounas/homer',
 'cbaziotis/ekphrasis',
 'unitaryai/detoxify',
 'OmkarPathak/pyresparser',
 'openvinotoolkit/nncf',
 'suragnair/seqGAN',
 'sheepzh/poetry',
 'mila-iqia/babyai',
 'ICLRandD/Blackstone',
 'cdqa-suite/cdQA',
 'HKUST-KnowComp/R-Net',
 'PaddlePaddle/RocketQA',
 'yongzhuo/Macropodus',
 'gutfeeling/word_forms',
 'HUANGZHIHAO1994/weibo-analysis-and-visualization',
 'timbmg/Sentence-VAE',
 'mit-han-lab/lite-transformer',
 'princeton-nlp/DensePhrases',
 'google-research/long-range-arena',
 'neuspell/neuspell',
 'thunlp/OpenHowNet',
 'tasdikrahman/vocabulary',
 'thunlp/OpenAttack',
 'MIND-Lab/OCTIS',
 'philipperemy/stanford-openie-python',
 'monologg/KoELECTRA',
 'shuaihuaiyi/QA',
 'primeqa/primeqa',
 'rinnakk/japanese-pretrained-models',
 'underneathall/pinferencia',
 'airalcorn2/Deep-Semantic-Similarity-Model',
 'stanfordnlp/python-stanford-corenlp',
 'graphbrain/graphbrain',
 'pemistahl/lingua-py',
 'google-research/bigbird',
 'subho406/OmniNet',
 'griptape-ai/griptape',
 'elbayadm/attn2d',
 'salesforce/matchbox',
 'PhantomInsights/subreddit-analyzer',
 'PhantomInsights/mexican-government-report',
 'allenai/allennlp-models',
 'sb-ai-lab/LightAutoML',
 'AnubhavGupta3377/Text-Classification-Models-Pytorch',
 'zhang17173/Event-Extraction',
 '1033020837/Basic4AI',
 'PracticingMan/chinese_text_cnn',
 'ChenglongChen/kaggle-HomeDepot',
 'alvations/sacremoses',
 'huggingface/datasets-server',
 'EagleW/PaperRobot',
 'keras-team/keras-nlp',
 'Ki6an/fastT5',
 'proycon/pynlpl',
 'koaning/whatlies',
 'jina-ai/examples',
 'imgarylai/bert-embedding',
 'titipata/pubmed_parser',
 'ematvey/hierarchical-attention-networks',
 'webis-de/small-text',
 'google-research/prompt-tuning',
 'PrithivirajDamodaran/Styleformer',
 'jiaeyan/Jiayan',
 'neuml/codequestion',
 'ayoungprogrammer/nlquery',
 'michaelthwan/searchGPT',
 'EricFillion/happy-transformer',
 'voidful/TextRL',
 'Cartus/AGGCN',
 'zzy99/epidemic-sentence-pair',
 'SCIR-HI/Med-ChatGLM',
 'mandarjoshi90/coref',
 'pochih/RL-Chatbot',
 'jjangsangy/ExplainToMe',
 'allenai/deep_qa',
 'ufal/neuralmonkey',
 'erickrf/nlpnet',
 'microsoft/tutel',
 'jerryji1993/DNABERT',
 'vas3k/infomate.club',
 'jkkummerfeld/text2sql-data',
 'ShannonAI/glyce',
 'aimi-cn/AILearners',
 'yxuansu/SimCTG',
 'OpenBMB/CPM-Live',
 'datawhalechina/daily-interview',
 'didi/ChineseNLP',
 'Unstructured-IO/unstructured',
 'chizhu/KGQA_HLM',
 'totalgood/nlpia',
 'akoumjian/datefinder',
 'neilgupta/Sherlock',
 'fhamborg/Giveme5W1H',
 'kevinlu1248/pyate',
 'yunwei37/COVID-19-NLP-vis',
 'maxim5/cs224n-2017-winter',
 'EvanLi/programming-book-3',
 'uiuing/varbook',
 'dair-ai/dair-ai.github.io',
 'datquocnguyen/RDRPOSTagger',
 'JasonKessler/Scattertext-PyData',
 'RevanthRameshkumar/CRD3',
 'deep-diver/EN-FR-MLT-tensorflow',
 'xiamx/fastText',
 'Botfuel/botfuel-dialog',
 'IlyaGusev/tgcontest',
 'narender-rk10/MyProctor.ai-AI-BASED-SMART-ONLINE-EXAMINATION-PROCTORING-SYSYTEM',
 '1712n/yachay-public',
 'fdasilva59/Udacity-Natural-Language-Processing-Nanodegree',
 'Yale-LILY/TutorialBank',
 'stephenleo/stripnet',
 'ansegura7/NLP',
 'ahmedbesbes/How-to-mine-newsfeed-data-and-extract-interactive-insights-in-Python',
 'ben-aaron188/rgpt3',
 'MLH-Fellowship/Social-BERTerfly',
 'usyiyi/nlp-py-2e-zh',
 'lunayach/visNER',
 '21han/nlp_qa_project',
 'ceshine/textrank_demo',
 'wayfair-incubator/extra-model',
 'pemagrg1/NLP-Flask-Website',
 'knadh/indic.page',
 'hazemabdelkawy/QuranGPT',
 'mneedham/neo4j-himym',
 'ShuHuang/batterydatabase',
 'NoHeartPen/NonJishoKei',
 'dzieciou/pystempel',
 'AnthonySigogne/web-search-engine-ui',
 'lunarwhite/covid-social-analysis',
 'Prakhar-FF13/Toxic-Comments-Classification',
 'MilanBreuno/AI-Content-Generator-Using-GPT-3-ACG',
 'iamyajat/HypeLinks-API',
 'abhi1nandy2/EMNLP-2021-Findings',
 'yshane-ai/yangxx_ML',
 'gagolews/stringx',
 'algonell/ipo-miner',
 'wynshiter/nlp_demo',
 'prachijain136/MCQ_Generator',
 'aasouzaconsult/Cientista-de-Dados',
 'lapolonio/machine-learning-for-nlp-guide',
 'ardauzunoglu/edebizeka',
 'lgessler/microbert',
 'AR621/Summ3r-y',
 'mohdsanadzakirizvi/IPLbot',
 'TigerGraph-DevLabs/RASA',
 'ShubhamDeodhar/CheckMate',
 'paladin-t/joan_demo',
 'rozester/Twitter-Sentiment-Analyzer',
 'howardvickers/resume-match',
 'anebz/eu-sim',
 'iszhaoxin/my-notebook',
 'juliasilge/ibm-ai-day',
 'piggywolfstudio/BookLibrary',
 'motazsaad/NLP-ICTS6361',
 'ivanliu1989/SwiftKey-Natural-language',
 'shizuo-kaji/TutorialTopologicalDataAnalysis',
 'hjlopes/sagemaker-sentiment-analysis',
 'hpzhao/nlp-metrics',
 'siat-nlp/NLP-docs',
 'mwt/econ-ipsum',
 'ruarq/ukraine-war-heatmap',
 'catherinesyeh/attention-viz',
 'shivam1808/AI-based-Virtual-Assistant',
 'zfsang-zz/CharacterGo',
 'm-newhauser/distilbert-senator-tweets',
 'd009/EstNLP',
 'plandes/deepnlp',
 'cosimameyer/nlp-rladies-bergen',
 'nlp-with-transformers/website',
 'ispasic/FlexiTerm-Java',
 'palomapiot/early',
 'Barqawiz/aind2-nlp-translation',
 'yanshengjia/link',
 'SkBlaz/autobot',
 'lopentu/nlp_web',
 'ammarasmro/Kurdish-Language',
 'bloomsburyai/ctrlf-tutorial',
 'JustinGOSSES/geoVec-playground',
 'nlposs/nlposs.github.io',
 'IgorWounds/speaking_with_plato',
 '2knal/Honesty',
 'alphadl/inspiring_papers',
 'anweasha/Text-Summarizer',
 'mkucz95/recommendation_engine',
 'ADHIKSHA/Resume-Parser',
 'InterruptorPt/misonia-hip-hop-tuga',
 'peterbozso/ULIS.NET',
 'hsingyin/flask-demo',
 'deepset-ai/haystack-home',
 'hscspring/hscspring.github.io',
 'mzmmoazam/kashmiri_dataset',
 'pln-fing-udelar/humor',
 'AliMorty/Text_Summerization',
 'SSusantAchary/Data-Annotator-for-SpaCy',
 'ravi3222/sentiment-analyzer007',
 'thejeswi/BobGoesToJail',
 'spujadas/coursera-ddp-shiny',
 'eveskew/rap_album_sentiment_analysis',
 'hemantkarekar/MyProctorAI',
 'CompNet/Nerwip',
 'lihanghang/TecRoom',
 'prakhar21/Hindi-Transliteration',
 'wang-h/LightTagger',
 'baaraban/pytorch_ner',
 'fw-tools/fwnl',
 'Rostlab/LocText',
 'akhatua2/Kaizen',
 'nd-z/modemo',
 'multilingual-dh/multilingual-dh.github.io',
 'krishanu-2001/Edgar-Database-Analyser',
 'ARKEnsae/TweetEmbedding',
 'rosette-api/php',
 'davidchengo/d3_cartogram_census_nlp',
 'myedibleenso/odin-tutorial-old',
 'asc-csa/Bilingual-Text-Analysis',
 'Ashishkumar-hub/Speech-to-text-using-speech_recognition-',
 'ishtiaque05/visualize-parts-of-speech',
 'darenr/gender-bias',
 'nerdimite/sentiment-analysis-deployment',
 'Kamomille/WebScrapping_Supermarket',
 'johnoseni1/NLP-video-player',
 'apache/incubator-nlpcraft-website',
 'knowledge-nlp/aaai2023',
 'tastyminerals/nn_search2',
 'rjjfox/disaster-response-classification',
 'Kartikaggarwal98/Propaganda_Detection-NLP4IF',
 'pranshurastogi29/Data-science-interview-assignments',
 'Ayshine/Artificial-Intelligence',
 'DaveSmith227/mastermind-gpt2',
 'moritztng/hedgy',
 'boostcampaitech3/level2-data-annotation_nlp-level2-nlp-09',
 'ykpgrr/Hate-Speech-Detection',
 'maxim5/cs224n-2019-winter',
 'erwanm/clg-authorship-analytics',
 'uragirii/Friends-Generator',
 'kaushal-py/QuizSource',
 'MichiganNLP/lifeqa',
 'Ashishkumar-hub/Text-to-Speech-using-gTTS',
 'argrecsys/arg-miner',
 '1475963/sentence-boundary-detection',
 'kudacall/nlpTutorial',
 'sfeng77/wordPrediction',
 'Eajack/NLP-Competition-DesignPattern',
 'sheepcloner/en_dictionaries_parsers',
 'ebenezerdon/ml5js-nlp',
 'HebrewNLP/docs.hebrew-nlp.co.il',
 'brajendra25/AI-ChatBot',
 'detsutut/ama-bot',
 'ADHIKSHA/Essay-Grading-IELTS',
 'broepke/DATA104',
 'omarsar/friendly_nlp',
 'zjy-T/Applied-Data-Science-with-Python-Specialization---Coursera',
 'DeepanNarayanaMoorthy/What-Does-BLM-Say',
 'imartinezl/data-science-roles',
 'ansegura7/DSL_Analysis',
 'Chinmaym49/BE_Project',
 'daniel-hain/ML_course_maastricht',
 'wwsalmon/rl-timeline',
 'winkjs/sentimental',
 'MichiganNLP/textgraphs',
 'jamesohortle/japanese-utilities',
 'MarketingPipeline/Conversations',
 'Jairus313/Text_Summarization',
 'sudoberlin/NLP_ND',
 'Hishok/Impact-of-News-Headlines-on-Stock-Indices',
 'AmirSafi/twitter-hate-speech-identification',
 'greenelab/text_mined_hetnet_manuscript',
 'markusmobius/go-htmldate',
 'memori-ai/memori-webcomponent',
 'mzsrtgzr2/machine_learning_apps_categotries',
 'allenai/allennlp-gallery',
 'vmallela0/COVerage',
 'Dibyakanti/AutoTNLI-code',
 'wj-Mcat/transformer',
 'giellalt/lang-sms',
 'jaydenwindle/mitie-gui',
 'jinmang2/Awesome-Papers',
 'oyahiroki/nlp4j',
 'MahalavanyaSriram/Natural-Language-Processing-with-Disaster-Tweets',
 'mcti-sefip/NLP-MCTI-PPF',
 'salehmontazeran/nlp-project',
 'dasdristanta13/NLP_work',
 'kelseymour/Chinese-NLP_Buddhist-Texts',
 'Azeemaj101/NLP_ChatBot',
 'prakhar-ai/InstantMD'
]

headers = {"Authorization": f"token {github_token}", "User-Agent": github_username}

if headers["Authorization"] == "token " or headers["User-Agent"] == "":
    raise Exception(
        "You need to follow the instructions marked TODO in this script before trying to use it"
    )


def github_api_request(url: str) -> Union[List, Dict]:
    response = requests.get(url, headers=headers)
    response_data = response.json()
    if response.status_code != 200:
        raise Exception(
            f"Error response from github api! status code: {response.status_code}, "
            f"response: {json.dumps(response_data)}"
        )
    return response_data


def get_repo_language(repo: str) -> str:
    url = f"https://api.github.com/repos/{repo}"
    repo_info = github_api_request(url)
    if type(repo_info) is dict:
        repo_info = cast(Dict, repo_info)
        if "language" not in repo_info:
            raise Exception(
                "'language' key not round in response\n{}".format(json.dumps(repo_info))
            )
        return repo_info["language"]
    raise Exception(
        f"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}"
    )


def get_repo_contents(repo: str) -> List[Dict[str, str]]:
    url = f"https://api.github.com/repos/{repo}/contents/"
    contents = github_api_request(url)
    if type(contents) is list:
        contents = cast(List, contents)
        return contents
    raise Exception(
        f"Expecting a list response from {url}, instead got {json.dumps(contents)}"
    )


def get_readme_download_url(files: List[Dict[str, str]]) -> str:
    """
    Takes in a response from the github api that lists the files in a repo and
    returns the url that can be used to download the repo's README file.
    """
    for file in files:
        if file["name"].lower().startswith("readme"):
            return file["download_url"]
    return ""


def process_repo(repo: str) -> Dict[str, str]:
    """
    Takes a repo name like "gocodeup/codeup-setup-script" and returns a
    dictionary with the language of the repo and the readme contents.
    """
    contents = get_repo_contents(repo)
    readme_download_url = get_readme_download_url(contents)
    if readme_download_url == "":
        readme_contents = ""
    else:
        readme_contents = requests.get(readme_download_url).text
    return {
        "repo": repo,
        "language": get_repo_language(repo),
        "readme_contents": readme_contents,
    }


def scrape_github_data() -> List[Dict[str, str]]:
    """
    Loop through all of the repos and process them. Returns the processed data.
    """
    return [process_repo(repo) for repo in REPOS]


if __name__ == "__main__":
    data = scrape_github_data()
    json.dump(data, open("data.json", "w"), indent=1)
